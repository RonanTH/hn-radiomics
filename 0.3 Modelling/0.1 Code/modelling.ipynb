{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules & Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\r\n",
    "%autoreload 2\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General modules & loading data\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import pingouin as pg\r\n",
    "import os\r\n",
    "from tqdm import tqdm, tqdm_gui\r\n",
    "\r\n",
    "from wrapperfunctions import *\r\n",
    "\r\n",
    "# Model Modules\r\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.linear_model import Lasso\r\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 30\r\n",
    "pd.options.display.max_columns = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_timepoint_wrapper(data, clf,timepoints,clinical_model=False, silent=True):\r\n",
    "\r\n",
    "    times = timepoints\r\n",
    "    \r\n",
    "    results = {'basic': [apply_single_clf(clf, data,save_path = confusion_saves, timepoint=t, apply_feature_selection=False, bagging=False, silent=silent) for t in times],\r\n",
    "                'feature_selection': [apply_single_clf(clf, data,save_path = confusion_saves, timepoint=t, apply_feature_selection=True, bagging=False, silent=silent) for t in times],\r\n",
    "                'bagging':[apply_single_clf(clf, data,save_path = confusion_saves, timepoint=t, apply_feature_selection=False, bagging=True, silent=silent) for t in times],\r\n",
    "                }\r\n",
    "\r\n",
    "    df = pd.DataFrame()\r\n",
    "\r\n",
    "    for i in list(results.keys()):\r\n",
    "        for j in range(len(times)):\r\n",
    "            df=df.append(results[i][j]['test_result']['results_df']).reset_index(drop=True)\r\n",
    "            if not clinical_model:\r\n",
    "                plot_km(results[i][j],data,folder='Naive Bayes',save_path = kaplan_saves)\r\n",
    "            \r\n",
    "    df.sort_values(by='timepoint',inplace=True)\r\n",
    "    \r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_timepoint_wrapper(data, clf,clinical_model=False, silent=True, reps=1):\r\n",
    "\r\n",
    "    \r\n",
    "    results = {'basic': apply_multi_clf(clf, data,repeat=reps, apply_feature_selection=False, bagging=False, silent=silent) ,\r\n",
    "                'feature_selection': apply_multi_clf(clf, data,repeat=reps, apply_feature_selection=True, bagging=False, silent=silent),\r\n",
    "                'bagging':apply_multi_clf(clf, data,repeat=reps, apply_feature_selection=False, bagging=True, silent=silent),\r\n",
    "                }\r\n",
    "\r\n",
    "    df = pd.DataFrame()\r\n",
    "\r\n",
    "    for i in list(results.keys()):\r\n",
    "        df=df.append(results[i]['results_df']).reset_index(drop=True)\r\n",
    "        # if not clinical_model:\r\n",
    "        #     plot_km(results[i],data,folder='Naive Bayes',save_path = kaplan_saves)\r\n",
    "            \r\n",
    "    # df.sort_values(by='timepoint',inplace=True)\r\n",
    "    \r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GaussianNB: 100%|██████████| 100/100 [00:01<00:00, 51.10it/s]\n",
      "GaussianNB: 100%|██████████| 100/100 [00:06<00:00, 15.69it/s]\n",
      "GaussianNB: 100%|██████████| 100/100 [00:44<00:00,  2.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>mode</th>\n      <th>accuracy</th>\n      <th>AUC</th>\n      <th>PR_score</th>\n      <th>f1_score</th>\n      <th>fb_score</th>\n      <th>MCC_Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GaussianNB</td>\n      <td>base</td>\n      <td>0.500000</td>\n      <td>0.574219</td>\n      <td>0.341270</td>\n      <td>0.181818</td>\n      <td>0.294118</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GaussianNB</td>\n      <td>feature selection</td>\n      <td>0.777778</td>\n      <td>0.742188</td>\n      <td>0.430208</td>\n      <td>0.333333</td>\n      <td>0.416667</td>\n      <td>0.236228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GaussianNB</td>\n      <td>bagging</td>\n      <td>0.666667</td>\n      <td>0.574219</td>\n      <td>0.353535</td>\n      <td>0.250000</td>\n      <td>0.357143</td>\n      <td>0.125000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "        model               mode  accuracy       AUC  PR_score  f1_score  \\\n0  GaussianNB               base  0.500000  0.574219  0.341270  0.181818   \n1  GaussianNB  feature selection  0.777778  0.742188  0.430208  0.333333   \n2  GaussianNB            bagging  0.666667  0.574219  0.353535  0.250000   \n\n   fb_score  MCC_Score  \n0  0.294118   0.000000  \n1  0.416667   0.236228  \n2  0.357143   0.125000  "
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\r\n",
    "multi_timepoint_wrapper(split_data,clf,reps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data & Applying Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Perform Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\r\n",
    "\r\n",
    "project_root =  os.path.dirname(os.path.dirname(cwd))\r\n",
    "feat_output_path = os.path.join(project_root,'0.1 Feature Extraction/0.2 Outputs/0.1 Extracted Features')\r\n",
    "\r\n",
    "output_path = os.path.join(project_root, '0.3 Modelling/0.2 Outputs')\r\n",
    "scores_saves = os.path.join(output_path, '0.1 Scores')\r\n",
    "confusion_saves = os.path.join(output_path, '0.2 Confusion Matrices')\r\n",
    "kaplan_saves = os.path.join(output_path, '0.3 Kaplan Meier Graphs')\r\n",
    "split_eval_saves = os.path.join(output_path, '0.4 Split Tests')\r\n",
    "\r\n",
    "clinical_data_path = os.path.join(project_root, '0.4 Clinical Data Processing/0.2 Data')\r\n",
    "\r\n",
    "\r\n",
    "image_feats = {'t1': pd.read_csv( os.path.join(feat_output_path, 'T1/Merged_Features_T1.csv')),'t2': pd.read_csv( os.path.join(feat_output_path, 'T2/Merged_Features_T2.csv'))}\r\n",
    "timepoints = ['t1','t2']\r\n",
    "rnd_state = 2\r\n",
    "split_data = {'t1':train_test_split(image_feats['t1'], random_state=rnd_state), 't2':train_test_split(image_feats['t2'], random_state=rnd_state)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse split for significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data_wb = pd.ExcelFile(os.path.join(clinical_data_path,'clinical_data.xlsx'))\r\n",
    "clinical_data = clinical_data_wb.parse('Clinical Data')\r\n",
    "\r\n",
    "clinical_data.drop(['Patient Data'], axis =1, inplace=True)\r\n",
    "clinical_data[\"split_group\"] = np.nan\r\n",
    "clinical_data.loc[clinical_data['PID'].isin(split_data['t1']['train_pids'].tolist()),'split_group'] = 'Train'\r\n",
    "clinical_data.loc[clinical_data['PID'].isin(split_data['t1']['test_pids'].tolist()),'split_group'] = 'Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform t-tests on numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_analysis = {}\r\n",
    "numeric_analysis['train_stats'] = clinical_data.loc[clinical_data['PID'].isin(split_data['t1']['train_pids'].tolist())].describe()\r\n",
    "numeric_analysis['test_stats'] = clinical_data.loc[clinical_data['PID'].isin(split_data['t1']['test_pids'].tolist())].describe()\r\n",
    "numeric_tests = pd.DataFrame()\r\n",
    "for i in list(clinical_data.select_dtypes(include=[np.number]).columns):\r\n",
    "    temp_res = pg.pairwise_ttests(data=clinical_data, dv=i, between='split_group')\r\n",
    "    temp_res['Variable']= [i]\r\n",
    "    numeric_tests=numeric_tests.append(temp_res,ignore_index=True)\r\n",
    "numeric_analysis['t_tests'] = numeric_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform chi-squared tests on categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_tests = {}\r\n",
    "categories = ['Gender','Diabetes','Drinker','Smoker','Final Stage','Ground Truth']\r\n",
    "for i in categories:\r\n",
    "    chi_tests[i] = pg.chi2_independence(data=clinical_data,x=i,y='split_group',correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirm_save = input(\"Are you sure you wish to save results (may overwrite existing results)? y/n\")\r\n",
    "\r\n",
    "if confirm_save =='y':\r\n",
    "    writer = pd.ExcelWriter(os.path.join(split_eval_saves,'split_evaluation.xlsx'), engine='xlsxwriter')\r\n",
    "\r\n",
    "    labels = ['Expected','Observed','Stats']\r\n",
    "\r\n",
    "\r\n",
    "    for key in chi_tests.keys():\r\n",
    "        row = 1\r\n",
    "        \r\n",
    "        for i in range(len(chi_tests[key])):\r\n",
    "            chi_tests[key][i].to_excel(writer, sheet_name=key,startrow=row , startcol=0)\r\n",
    "            worksheet = writer.sheets[key]\r\n",
    "            worksheet.write_string(row-1, 0, labels[i])\r\n",
    "\r\n",
    "            row = row + len(chi_tests[key][i].index)  + 3\r\n",
    "\r\n",
    "    row =1       \r\n",
    "    for i in list(numeric_analysis.keys()):\r\n",
    "        numeric_analysis[i].to_excel(writer, sheet_name='Numeric Analysis',startrow=row , startcol=0)\r\n",
    "        worksheet = writer.sheets['Numeric Analysis']\r\n",
    "        worksheet.write_string(row-1, 0, i)\r\n",
    "        row = row + len(numeric_analysis[i].index)  + 3\r\n",
    "            \r\n",
    "    writer.save()\r\n",
    "    writer.close()\r\n",
    "\r\n",
    "else:\r\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radiomics Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Timepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:21<00:00,  5.48s/it]\n"
     ]
    }
   ],
   "source": [
    "clfs = [GaussianNB(),\r\n",
    "        AdaBoostClassifier(),\r\n",
    "        GradientBoostingClassifier(),\r\n",
    "        xgb.XGBClassifier(objective='binary:logistic', eval_metric = 'auc', n_estimators= 100, learning_rate=0.01,use_label_encoder=False, max_depth=15, n_jobs=18)]\r\n",
    "\r\n",
    "rad_single_results = pd.DataFrame()\r\n",
    "\r\n",
    "for clf in tqdm(clfs):\r\n",
    "    rad_single_results =rad_single_results.append(single_timepoint_wrapper(split_data,clf,timepoints),ignore_index=True)\r\n",
    "rad_single_results.sort_values(['timepoint','model'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-timepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GaussianNB: 100%|██████████| 100/100 [00:01<00:00, 50.18it/s]\n",
      "GaussianNB: 100%|██████████| 100/100 [00:06<00:00, 14.84it/s]\n",
      "GaussianNB: 100%|██████████| 100/100 [00:41<00:00,  2.42it/s]\n",
      "AdaBoostClassifier: 100%|██████████| 100/100 [01:29<00:00,  1.12it/s]\n",
      "AdaBoostClassifier: 100%|██████████| 100/100 [00:20<00:00,  4.95it/s]\n",
      "AdaBoostClassifier: 100%|██████████| 100/100 [04:09<00:00,  2.49s/it]\n",
      "GradientBoostingClassifier: 100%|██████████| 100/100 [04:10<00:00,  2.50s/it]\n",
      "GradientBoostingClassifier: 100%|██████████| 100/100 [00:14<00:00,  6.84it/s]\n",
      "GradientBoostingClassifier: 100%|██████████| 100/100 [04:52<00:00,  2.92s/it]\n",
      "XGBClassifier: 100%|██████████| 100/100 [00:33<00:00,  3.02it/s]\n",
      "XGBClassifier: 100%|██████████| 100/100 [00:17<00:00,  5.83it/s]\n",
      "XGBClassifier: 100%|██████████| 100/100 [09:21<00:00,  5.62s/it]\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 100\r\n",
    "learning_rate=0.01\r\n",
    "\r\n",
    "classifiers = [GaussianNB(), AdaBoostClassifier(),GradientBoostingClassifier(n_estimators=n_estimators,learning_rate=learning_rate),xgb.XGBClassifier(objective='binary:logistic',eval_metric = 'logloss', n_estimators= n_estimators, learning_rate=learning_rate,use_label_encoder=False, max_depth=10, n_jobs=18)]\r\n",
    "\r\n",
    "rad_multi_results = pd.DataFrame()\r\n",
    "full_res =[]\r\n",
    "for clf in classifiers:\r\n",
    "    temp_res = multi_timepoint_wrapper(split_data,clf,reps=100)\r\n",
    "    # full_res.append(temp_res)\r\n",
    "    rad_multi_results=rad_multi_results.append(temp_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "suv_data = clinical_data_wb.parse('SUVs')\r\n",
    "suv_data.drop(['Patient Data','T1 Date', 'T2 Date'], axis =1, inplace=True)\r\n",
    "split_suv_data = {'t1':clinical_train_test_split(suv_data[['PID', 'T1 Toncil SUV', 'T1 Liver SUV', 'T1 Normalised Toncil SUV', 'Response', 'Ground Truth']],random_state=rnd_state),\r\n",
    "                  't2':clinical_train_test_split(suv_data[['PID', 'T2 Toncil SUV', 'T2 Liver SUV', 'T2 Normalised Toncil SUV', '% Change in Toncil SUV', 'Response', 'Ground Truth']],random_state=rnd_state)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "clfs = [GaussianNB(),\r\n",
    "        AdaBoostClassifier(),\r\n",
    "        GradientBoostingClassifier(),\r\n",
    "        xgb.XGBClassifier(objective='binary:logistic', eval_metric = 'auc', n_estimators= 100, learning_rate=0.01,use_label_encoder=False, max_depth=15, n_jobs=18)]\r\n",
    "\r\n",
    "clinical_single_timepoint = pd.DataFrame()\r\n",
    "\r\n",
    "for clf in tqdm(clfs):\r\n",
    "    clinical_single_timepoint =clinical_single_timepoint.append(single_timepoint_wrapper(split_suv_data,clf,timepoints=timepoints,clinical_model=True),ignore_index=True)\r\n",
    "clinical_single_timepoint.sort_values(['timepoint','model'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GaussianNB: 100%|██████████| 100/100 [00:01<00:00, 85.08it/s]\n",
      "GaussianNB: 100%|██████████| 100/100 [00:01<00:00, 66.14it/s]\n",
      "GaussianNB: 100%|██████████| 100/100 [00:18<00:00,  5.51it/s]\n",
      "AdaBoostClassifier: 100%|██████████| 100/100 [00:14<00:00,  6.74it/s]\n",
      "AdaBoostClassifier: 100%|██████████| 100/100 [00:15<00:00,  6.66it/s]\n",
      "AdaBoostClassifier: 100%|██████████| 100/100 [02:42<00:00,  1.62s/it]\n",
      "GradientBoostingClassifier: 100%|██████████| 100/100 [00:10<00:00,  9.32it/s]\n",
      "GradientBoostingClassifier: 100%|██████████| 100/100 [00:10<00:00,  9.49it/s]\n",
      "GradientBoostingClassifier: 100%|██████████| 100/100 [01:22<00:00,  1.21it/s]\n",
      "XGBClassifier: 100%|██████████| 100/100 [00:12<00:00,  7.85it/s]\n",
      "XGBClassifier: 100%|██████████| 100/100 [00:12<00:00,  7.80it/s]\n",
      "XGBClassifier: 100%|██████████| 100/100 [04:54<00:00,  2.94s/it]\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 100\r\n",
    "learning_rate=0.01\r\n",
    "\r\n",
    "classifiers = [GaussianNB(), AdaBoostClassifier(),GradientBoostingClassifier(n_estimators=n_estimators,learning_rate=learning_rate),xgb.XGBClassifier(objective='binary:logistic',eval_metric = 'logloss', n_estimators= n_estimators, learning_rate=learning_rate,use_label_encoder=False, max_depth=10, n_jobs=18)]\r\n",
    "\r\n",
    "clinicla_multi_results = pd.DataFrame()\r\n",
    "full_res =[]\r\n",
    "for clf in classifiers:\r\n",
    "    temp_res = multi_timepoint_wrapper( split_suv_data,clf, reps=100)\r\n",
    "    # full_res.append(temp_res)\r\n",
    "    clinicla_multi_results=clinicla_multi_results.append(temp_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirm_save = input(\"Are you sure you wish to save results (may overwrite existing results)? y/n\")\r\n",
    "\r\n",
    "if confirm_save =='y':\r\n",
    "\r\n",
    "    writer = pd.ExcelWriter(os.path.join(scores_saves,'classifier_scores.xlsx'), engine='xlsxwriter')\r\n",
    "\r\n",
    "    rad_single_results.to_excel(writer, sheet_name='Radiomics ST Models')\r\n",
    "    rad_multi_results.to_excel(writer, sheet_name='Radiomics MT Models')\r\n",
    "    clinical_single_timepoint.to_excel(writer, sheet_name='Clinical ST Models')\r\n",
    "    clinicla_multi_results.to_excel(writer, sheet_name='Clinical MT Models')\r\n",
    "    writer.save()\r\n",
    "    \r\n",
    "else:\r\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of wrapperfunctions failed: Traceback (most recent call last):\n",
      "  File \"d:\\Sharepoint\\OneDrive - University College Dublin\\0.1 Current Year\\5th Year\\Project\\0.04 Radiomics Approach\\.win_radiomics_venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"d:\\Sharepoint\\OneDrive - University College Dublin\\0.1 Current Year\\5th Year\\Project\\0.04 Radiomics Approach\\.win_radiomics_venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"X:\\Python 64\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"X:\\Python 64\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 916, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"d:\\Sharepoint\\OneDrive - University College Dublin\\0.1 Current Year\\5th Year\\Project\\0.04 Radiomics Approach\\0.3 Modelling\\0.1 Code\\wrapperfunctions.py\", line 314\n",
      "    'groups' : [ 'Remission' if x ==0 else 'progression' if x==1 for x in model_output['predictions']],\n",
      "                                                                 ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# clfs = [GaussianNB(),\r\n",
    "#         AdaBoostClassifier(),\r\n",
    "#         GradientBoostingClassifier(),\r\n",
    "#         xgb.XGBClassifier(objective='binary:logistic', eval_metric = 'auc', n_estimators= 100, learning_rate=0.01,use_label_encoder=False, max_depth=15, n_jobs=18)]\r\n",
    "\r\n",
    "# clinical_results = pd.DataFrame()\r\n",
    "\r\n",
    "# for clf in tqdm(clfs):\r\n",
    "#     clinical_results =clinical_results.append(single_timepoint_wrapper(split_suv_data,clf, clinical_model=True),ignore_index=True)\r\n",
    "# clinical_results.sort_values(['timepoint','model'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = Lasso()\r\n",
    "\r\n",
    "# xgb_results = {'basic': [apply_single_clf(clf, split_data, timepoint=t, apply_feature_selection=False, bagging=False, silent=True) for t in timepoints],\r\n",
    "#               'feature_selection': [apply_single_clf(clf, split_data, timepoint=t, apply_feature_selection=True, bagging=False, silent=True) for t in timepoints],\r\n",
    "#               'bagging':[apply_single_clf(clf, split_data, timepoint=t, apply_feature_selection=False, bagging=True, silent=True) for t in timepoints],\r\n",
    "#               }\r\n",
    "\r\n",
    "# xgb_df = pd.DataFrame()\r\n",
    "\r\n",
    "# for i in list(xgb_results.keys()):\r\n",
    "#     for j in range(len(timepoints)):\r\n",
    "#         xgb_df=xgb_df.append(xgb_results[i][j]['test_result']['results_df']).reset_index(drop=True)\r\n",
    "#         plot_km(xgb_results[i][j],split_data,folder='SVC',save_path = kaplan_saves)\r\n",
    "# xgb_df.sort_values(by='timepoint',inplace=True)\r\n",
    "      \r\n",
    "# display(xgb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = pd.ExcelWriter(os.path.join(split_eval_saves,'split_evaluation.xlsx'), engine='xlsxwriter')\r\n",
    "\r\n",
    "# mann_whit_tests = \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# single_timepoint_results.to_excel(writer, sheet_name='Radiomics ST Models')\r\n",
    "# results.to_excel(writer, sheet_name='Radiomics MT Models')\r\n",
    "# clinical_results.to_excel(writer, sheet_name='Clinical Models')\r\n",
    "# writer.save()\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6  ('.win_radiomics_venv': venv)",
   "name": "pythonjvsc74a57bd097181e6aaff5971fbf09456615bf7118e041975beba5dd944c64901d0c02f5c4"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "97181e6aaff5971fbf09456615bf7118e041975beba5dd944c64901d0c02f5c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}